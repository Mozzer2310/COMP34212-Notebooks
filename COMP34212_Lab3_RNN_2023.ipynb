{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mozzer2310/COMP34212-Notebooks/blob/main/COMP34212_Lab3_RNN_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpnRb1rNp6mv"
      },
      "source": [
        "Recurrent Neural Networks (RNN): Simple RNNs and LSTMs\n",
        "=========\n",
        "\n",
        "In this tutorial we will learn how to use Recurremt Neural Networks (RNNs) for text processing.\n",
        "The first example uses a simple RNN to generate the characters in a small text corpus (Alice in Wonderland novel).\n",
        "We will then look at the Long Short Term Memory (LSTM) networks, for more advanced recurrent models. The LSTM will be tested on a sentiment analysis corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjaUzq1ip6m1"
      },
      "source": [
        "1 - Simple RNNs to Generate text\n",
        "=========\n",
        "\n",
        "This exmple will use a simple RNN with one layer of hidden and recurrent units. \n",
        "\n",
        "Let's first import the usual keras and utility functions. This will include the first importing and use of the SimpleRNN recurrent layer type in Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx1fsJ45p6nA"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdP2MSmp6nP"
      },
      "source": [
        "**Loading text file and converting to clean text**\n",
        "\n",
        "This code will read the file \"alice_in_wonderland.txt\". This file is available for download here http://www.gutenberg.org/files/11/11-0.txt (and can also be downloaded from Balckboard ). \n",
        "\n",
        "The code will do some preliminary cleanup of the text (e.g. removing non-ASCII characters and line breaks) and write all words in the variable called \"whole_text\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtQ15qnCp6nS",
        "outputId": "a963bdb9-5c94-4ec5-df8b-d8851501828b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text file...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading text file...\")\n",
        "\n",
        "INPUT_FILE = \"alice_in_wonderland.txt\"\n",
        "\n",
        "fin = open(INPUT_FILE, 'rb')\n",
        "lines = []\n",
        "for line in fin:\n",
        "    line = line.strip().lower()\n",
        "    line = line.decode(\"ascii\", \"ignore\")\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "fin.close()\n",
        "whole_text = \" \".join(lines)\n",
        "\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRl-KiBp6nh"
      },
      "source": [
        "**Characters look-up table**\n",
        "\n",
        "This code will create the look-up table from the 42 characters to integer IDs, and viceversa. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzNWK_g1p6nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504b7618-f1b7-42ca-d1af-b273e68ecd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing characters look-up table...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Preparing characters look-up table...\")\n",
        "\n",
        "chars = set([c for c in whole_text])\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VKYsImp6nv"
      },
      "source": [
        "**Input text and output character**\n",
        "\n",
        "To create the input text, the code will step through the whole text by a number of characters defined by the __STEP__ variable (1 in our case) and then extract a set of characters whose size is determined by the __SEQLEN__ variable (10 in our case). The next character after the extracted characters is the output label, i,e. the next character to predict.\n",
        "\n",
        "Here is an example of the input/output for the part of text starting as\" it turned into a pig\" \n",
        "\n",
        "   INPUT (10)    ->   OUTPUT (1)\n",
        "- \"it turned \"   ->   i\n",
        "- \"t turned i\"   ->   n\n",
        "- \" turned in\"   ->   t\n",
        "- \"turned int\"   ->   o\n",
        "- \"urned into\"   ->\n",
        "- \"rned into \"   ->   a\n",
        "- \"ned into a\"   ->\n",
        "- \"ed into a \"   ->   p\n",
        "- \"d into a p\"   ->   i\n",
        "- \" into a pi\"   ->   g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyS8tFKsp6nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9dc365-f5f7-4e78-c1f9-fa329da52cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating input and label text...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating input and label text...\")\n",
        "\n",
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(whole_text) - SEQLEN, STEP):\n",
        "    input_chars.append(whole_text[i:i + SEQLEN])\n",
        "    label_chars.append(whole_text[i + SEQLEN])\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaTGaKHap6n_"
      },
      "source": [
        "**Vectorisation of the input and output**\n",
        "\n",
        "This preprares the input and output vectors of the training set. \n",
        "The input vector uses one-hot encoding of the __SEQLEN__ (10) characters present in the input text segment, times the __nb_chars__ (42) possible characters.\n",
        "The output label uses one-hot encoding for the activation of a single character out of the __nb_chars__ number of units/characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssALtUABp6oC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720483b5-773d-4d36-93f9-472540fd2363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing input and label text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-4c1b68e28321>:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
            "<ipython-input-5-4c1b68e28321>:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorizing input and label text...\")\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZKv1Xwep6oJ"
      },
      "source": [
        "**Simple RNN Model definition**\n",
        "\n",
        "This codes defines the key variables of the model and training, and creates the sequential model. The model consists of a simple recurrent neural network (RNN) with a hidden layer of 128 simple recurrent units.\n",
        "\n",
        "The __return_sequences__ is set to __False__ as the output only consists of one character, and not a sequence. \n",
        "The __unroll=True__ setting improves performance on the TensorFlow backend.\n",
        "The optimiser uses the __rmsprop__ for backpropagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZFdn5ynp6oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963dd4da-1ce7-43be-87a1-a91524bcc64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 128)               23424     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 54)                6966      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 54)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,390\n",
            "Trainable params: 30,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definition of the network and training hyperparameters\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "# Definition of the network topology, with simpleRNN hidden layer\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "#show the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrEyYv-hp6oR"
      },
      "source": [
        "**Training and testing of the model**\n",
        "\n",
        "The model is trained for __NUM_ITERATIONS__ (25) epochs and tested after each epoch, to allow us to monitor the improvement of the model performance in character prediction.\n",
        "\n",
        "The test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from the previous run as the new input, to generate another character. This is done for __NUM_PREDS_PER_EPOCH__ (100) steps. The completed string gives us an indication of the quality of the model's processing of English words (within the limited lexicon of Alice's novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKeEI_hup6oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c877411e-43ad-4475-84de-7beb9cd7e612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Iteration #: 0\n",
            "1236/1236 [==============================] - 42s 31ms/step - loss: 2.3404\n",
            "Generating from seed: th is it? \n",
            "th is it? alice and the the sore the the sore the the sore the the sore the the sore the the sore the the sore\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "1236/1236 [==============================] - 27s 22ms/step - loss: 2.0425\n",
            "Generating from seed: n of cours\n",
            "n of cours and and the said the said the said the said the said the said the said the said the said the said t\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "1236/1236 [==============================] - 26s 21ms/step - loss: 1.9353\n",
            "Generating from seed: g--catch h\n",
            "g--catch hares and the said the cateres and alice there the said the cateres and alice there the said the cate\n",
            "==================================================\n",
            "Iteration #: 3\n",
            "1236/1236 [==============================] - 18s 14ms/step - loss: 1.8550\n",
            "Generating from seed: use out of\n",
            "use out of the said the doon the said the doon the said the doon the said the doon the said the doon the said \n",
            "==================================================\n",
            "Iteration #: 4\n",
            "1236/1236 [==============================] - 16s 13ms/step - loss: 1.7900\n",
            "Generating from seed: the pictur\n",
            "the picturs the care the care the care the care the care the care the care the care the care the care the care\n",
            "==================================================\n",
            "Iteration #: 5\n",
            "1236/1236 [==============================] - 18s 15ms/step - loss: 1.7370\n",
            "Generating from seed: id not sne\n",
            "id not sne had the had the had the had the had the had the had the had the had the had the had the had the had\n",
            "==================================================\n",
            "Iteration #: 6\n",
            "1236/1236 [==============================] - 16s 13ms/step - loss: 1.6913\n",
            "Generating from seed: and waited\n",
            "and waited to her alice was the project gutenberg-tm election of the project gutenberg-tm election of the proj\n",
            "==================================================\n",
            "Iteration #: 7\n",
            "1236/1236 [==============================] - 16s 13ms/step - loss: 1.6529\n",
            "Generating from seed:  alice, an\n",
            " alice, and the dormouse so she was the dormouse so she was the dormouse so she was the dormouse so she was th\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "1236/1236 [==============================] - 17s 14ms/step - loss: 1.6192\n",
            "Generating from seed: neral clap\n",
            "neral claped of the was the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the dorm\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "1236/1236 [==============================] - 16s 13ms/step - loss: 1.5910\n",
            "Generating from seed: ts the jud\n",
            "ts the judy bead the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse th\n",
            "==================================================\n",
            "Iteration #: 10\n",
            "1236/1236 [==============================] - 14s 11ms/step - loss: 1.5659\n",
            "Generating from seed: u know, as\n",
            "u know, as it as it as it as it as it as it as it as it as it as it as it as it as it as it as it as it as it \n",
            "==================================================\n",
            "Iteration #: 11\n",
            "1236/1236 [==============================] - 14s 11ms/step - loss: 1.5441\n",
            "Generating from seed: leep. afte\n",
            "leep. after alice as she was not to be a little some of the toon the toon the toon the toon the toon the toon \n",
            "==================================================\n",
            "Iteration #: 12\n",
            "1236/1236 [==============================] - 14s 11ms/step - loss: 1.5241\n",
            "Generating from seed: em with la\n",
            "em with large for it was a little the rabbit and said the caterpillar to conter the rabbit and said the caterp\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.5066\n",
            "Generating from seed:  cat remar\n",
            " cat remarked alice was the project gutenberg-tm electronic work in the same alice was the project gutenberg-t\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.4906\n",
            "Generating from seed: n, and ali\n",
            "n, and alice was she was not all the caterpillar some the door of the works you deat down the caterpillar some\n",
            "==================================================\n",
            "Iteration #: 15\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.4764\n",
            "Generating from seed: rts were s\n",
            "rts were searly and she said the king the same alice she had not the dormouse she said the king the same alice\n",
            "==================================================\n",
            "Iteration #: 16\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.4633\n",
            "Generating from seed: e one, but\n",
            "e one, but the was a little some the mock turtle so states and her as she was so states and her as she was so \n",
            "==================================================\n",
            "Iteration #: 17\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.4522\n",
            "Generating from seed: ce of the \n",
            "ce of the project gutenberg-tm looked at the project gutenberg-tm looked at the project gutenberg-tm looked at\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.4406\n",
            "Generating from seed: ried. come\n",
            "ried. come of the said to herself of the said to herself of the said to herself of the said to herself of the \n",
            "==================================================\n",
            "Iteration #: 19\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.4310\n",
            "Generating from seed: y the sold\n",
            "y the soldo she was a longed at all reading about the project gutenberg-tm electronic work or she was the dorm\n",
            "==================================================\n",
            "Iteration #: 20\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.4213\n",
            "Generating from seed: ont like t\n",
            "ont like the mock turtle was not the mock turtle was not the mock turtle was not the mock turtle was not the m\n",
            "==================================================\n",
            "Iteration #: 21\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.4123\n",
            "Generating from seed: d up the o\n",
            "d up the other down the court again, and the work at all the rest of the works and the rabbit in a compling at\n",
            "==================================================\n",
            "Iteration #: 22\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.4049\n",
            "Generating from seed: headed, an\n",
            "headed, and all the rest of the words of the poor alice was the terms of the poor alice was the terms of the p\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "1236/1236 [==============================] - 15s 12ms/step - loss: 1.3965\n",
            "Generating from seed: ches are g\n",
            "ches are gring the rabbit had not the the should have say what a little thing all carreating and the thing a c\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "1236/1236 [==============================] - 14s 12ms/step - loss: 1.3889\n",
            "Generating from seed: dy has won\n",
            "dy has wonder as she said to herself looked at the work and dont as she said to herself looked at the work and\n"
          ]
        }
      ],
      "source": [
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "    \n",
        "    # testing model\n",
        "    # randomly choose a row from input_chars, then use it to \n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for i, ch in enumerate(test_chars):\n",
        "            Xtest[0, i, char2index[ch]] = 1\n",
        "        pred = model.predict(Xtest, verbose=0)[0]\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXjvnFK6p6og"
      },
      "source": [
        "This example shows how we can train a Simple RNN to predict the next charaters, using the information from the sequentail relationship between words in the training text.\n",
        "You can try the same code on a different, longer text, or try different hyperparameters.\n",
        "\n",
        "We are now ready to try a more complex RNN architecture, for more complex time series tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY5Pe1aCp6oi"
      },
      "source": [
        "2 - LSTM for Sentiment Analysis\n",
        "=========\n",
        "\n",
        "This example will show how to code the more complex RNN model of the Long Short Term Memmory (LSTM) network. This tutorial will use the example of sentiment analysis prediction.\n",
        "\n",
        "Let's start with the usual importing of Keras and utility modules. This will of course include the LSTM recurrent layer type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJqODAGwp6ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bd6848-4e71-4715-d92e-a7814c843aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialisation Done! Make sure you have initialised ntlk punkt\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Activation, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt') # download instruction required if pubkt nltk package not installed\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"Initialisation Done! Make sure you have initialised ntlk punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8a2MWghp6oq"
      },
      "source": [
        "Let's now define the main hyperparameter values and text corpus details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__8WvCXSp6ot"
      },
      "outputs": [],
      "source": [
        "INPUT_FILE = \"umich-sentiment-train.txt\"\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_LAYER_SIZE = 64\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpXCUzwep6oz"
      },
      "source": [
        "**Analysis of words in corpus**\n",
        "\n",
        "This code load the text from the UMICH labelled sentiment analysis corpus file, to identify the number of unique words (length of the __word_freq__ variable, i.e. 2313) and the max number of words in the longest sentence (__maxlen__ expected to be 42). These two variables will be needed later for the preaparation of the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E63yJkoMp6o1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c102e53b-d181-43f0-e979-72dfa6e908e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-85871faf8379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_recs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mftrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mftrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'umich-sentiment-train.txt'"
          ]
        }
      ],
      "source": [
        "# Read training data and generate vocabulary\n",
        "maxlen = 0\n",
        "word_freqs = collections.Counter()\n",
        "num_recs = 0\n",
        "\n",
        "ftrain = open(INPUT_FILE, 'rb')\n",
        "i = 0\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(b\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        word_freqs[word] += 1\n",
        "    num_recs += 1\n",
        "    i += 1\n",
        "ftrain.close()\n",
        "\n",
        "## Get some information about our corpus\n",
        "print (maxlen)            # 42\n",
        "print (len(word_freqs))   # 2313"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiqZt7Dlp6pB"
      },
      "source": [
        "**Unkown word and padding word and lookup tables**\n",
        "\n",
        "Knowing the number of unique words will permit the identification of the out of vocabulary (OOV) words, which can be replaced by the pseudo-word \"UNK\" (for unknown). When testing the network prediction, this will allow us to handle previously unseen OOV words.\n",
        "\n",
        "We can decide a fixed sequence length by truncating longer sentences to that length as appropriate (__MAX_SENTENCE_LENGTH = 40__). We also use the PAD to make shorter sentence equivalent to the fixed max length.\n",
        "\n",
        "Overall, we set our VOCABULARY_SIZE to 2002. i.e. 2,000 words from our vocabulary plus the UNK pseudo-word and the PAD pseudo word used for padding sentences up to the max length.\n",
        "\n",
        "We then create the look-up tables. Each row of input to the network is a sequence of word indices, where the indices are ordered by most frequent to least frequent word in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abPruGn0p6pD"
      },
      "outputs": [],
      "source": [
        "MAX_FEATURES = 2000\n",
        "MAX_SENTENCE_LENGTH = 40\n",
        "\n",
        "# 1 is UNK, 0 is PAD\n",
        "# We take MAX_FEATURES-1 featurs to accound for PAD\n",
        "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
        "word2index = {x[0]: i+2 for i, x in \n",
        "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
        "word2index[\"PAD\"] = 0\n",
        "word2index[\"UNK\"] = 1\n",
        "index2word = {v:k for k, v in word2index.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivjEnz11p6pI"
      },
      "source": [
        "**Prepare training sequences**\n",
        "\n",
        "The input sentences need to be converted to  word index sequences (inlcuding their padiing to the max sequecne length). \n",
        "No procesing required for the output, as this is a positive/negative binary sentiment output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA9RgxaDp6pK"
      },
      "outputs": [],
      "source": [
        "# convert sentences to sequences\n",
        "X = np.empty((num_recs, ), dtype=list)\n",
        "y = np.zeros((num_recs, ))\n",
        "i = 0\n",
        "ftrain = open(INPUT_FILE, 'rb')\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(b\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    seqs = []\n",
        "    for word in words:\n",
        "        if word in word2index:\n",
        "            seqs.append(word2index[word])\n",
        "        else:\n",
        "            seqs.append(word2index[\"UNK\"])\n",
        "    X[i] = seqs\n",
        "    y[i] = int(label)\n",
        "    i += 1\n",
        "ftrain.close()\n",
        "\n",
        "# Pad the sequences (left padded with zeros)\n",
        "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# Split input into training and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x4dQwv_p6pQ"
      },
      "source": [
        "**Model building and compilation**\n",
        "\n",
        "The codes defines the LSTM model topology, as per figure below. This is a stdnard recurrent network with one layer of LSTM units.\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MKE0uglp6pR"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_SENTENCE_LENGTH))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#show the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRy2C7YJp6pV"
      },
      "source": [
        "**Training of the model**\n",
        "\n",
        "Let's train our LSTM RNN.\n",
        "The model uses the binary cross-entropy loss function, suitable for binary value prediction. The Adam optimizer is one of the best and most frequently used general purpose optimizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDc6PUGfp6pX"
      },
      "outputs": [],
      "source": [
        "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTrIcuLp6pk"
      },
      "source": [
        "**Plotting of the results**\n",
        "\n",
        "The code below will produce accuracy and loss plots for training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1c-1aJFp6pl"
      },
      "outputs": [],
      "source": [
        "# plot loss and accuracy\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"accuracy\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_accuracy\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99eFGdup6pp"
      },
      "source": [
        "**Evaluation of the model**\n",
        "\n",
        "The model is evaluated against the full test set, printing the score and accuracy. \n",
        "The code also generates a few random sentences from the test set and prints the model's prediction, the label and the actual sentence. This will give us an idea of the quality of the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C57xXLt0p6pq"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(len(Xtest))\n",
        "    xtest = Xtest[idx].reshape(1,40)\n",
        "    ylabel = ytest[idx]\n",
        "    ypred = model.predict(xtest)[0][0]\n",
        "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
        "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQKK-orp6px"
      },
      "source": [
        "Conclusions\n",
        "-------------\n",
        "\n",
        "This exercise finishes our short deep leanring course. You know have a pracyicla understanding of how to train a simple MLP (multi-Layer Perceptron), CNNs (Concolutional Neural Networks) with different layers, a simple RNN (Recurrent Neural network) and the more complex LSTM (Long Short Term Neural Network). Well done!\n",
        "\n",
        "**Copyright (c)** 2022 Angelo Cangelosi, MIT License. Code and examples adapted from Gulli & Pal (2017) Deep Learning with Keras. Punkt Publishing. With the support of Wenjie Huang."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}